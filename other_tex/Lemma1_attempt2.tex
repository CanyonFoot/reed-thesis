\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

% Sensible defaults for lstlistings
\lstset{
  basicstyle=\footnotesize\ttfamily,
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  commentstyle=\bfseries\color{purple!40!black}
  frame=L,
  identifierstyle=\color{blue},
  keywordstyle=\bfseries\color{green!40!black},
  language=python,
  showstringspaces=false,
  stringstyle=\color{orange},
}

\title{\vspace{-3cm} Lemma 1 Proof Take 2}
\author{Canyon Foot}


\begin{document}
\maketitle

For binary random variables $X,Y,S$, we wish to show that for the two causal graphs, $G_1$, $G_2 = G_1 \backslash \{Y \rightarrow S\}$ we can construct corresponding probability functions $P_1$, $P_2$ such that $P_1(Y = y | X = x | S = 1) = P_2(Y = y | X = x | S = 1)$ for all $x, y \in \{0, 1\}$ but $P(Y = y | X = x) \neq P(Y = y | X = x)$ for some $x,y \in \{0,1\}$. This is exactly the condition that s-recoverability requires, so this example proves that $G_1$ is not s-recoverable.

\begin{tikzpicture}
\node[] (X) at (2,1) {$G_1$};
\node[shape=circle,draw=black] (X) at (0,0) {X};
\node[shape=circle,draw=black] (Y) at (4,0) {Y};
\node[shape=circle,draw=black] (S) at (2,-2) {S};

 \path [->] (X) edge node[left] {} (Y);
 \path [->] (Y) edge node[left] {} (S);

\node[] (X) at (8,1) {$G_2$};
\node[shape=circle,draw=black] (X) at (6,0) {X};
\node[shape=circle,draw=black] (Y) at (10,0) {Y};
\node[shape=circle,draw=black] (S) at (8,-2) {S};
 \path [->] (X) edge node[left] {} (Y);
\end{tikzpicture}


The graph structure gives us access to the following conditionals, which we have filled in with particular values.

For $P_1$, we have: \\

\begin{tabular}{llll|l|llllllllll}
\multicolumn{1}{l|}{$x$} & $P_1(X = x)$ &  & $x$ & $y$ & $P_1(Y = y | X = x)$ &  & \multicolumn{1}{l|}{$y$} & \multicolumn{1}{l|}{$s$} & $P_1(S = 1 | Y = y)$ &  &  &  &  &  \\ \cline{1-2} \cline{4-6} \cline{8-10}
\multicolumn{1}{l|}{0}   & $1/2$        &  & 0   & 0   & $1/2$                &  & \multicolumn{1}{l|}{1}   & \multicolumn{1}{l|}{1}   & $3/4$                &  &  &  &  &  \\
\multicolumn{1}{l|}{1}   & $1/2$        &  & 0   & 1   & $1/2$                &  & \multicolumn{1}{l|}{0}   & \multicolumn{1}{l|}{1}   & $1/4$                &  &  &  &  &  \\
                         &              &  & 1   & 0   & $1/2$                &  &                          &                          &                      &  &  &  &  &  \\
                         &              &  & 1   & 1   & $1/2$                &  &                          &                          &                      &  &  &  &  & 
\end{tabular}

And for $P_2$, we have: \\


\begin{tabular}{llll|l|lllllll}
\multicolumn{1}{l|}{$x$} & $P_2(X = x)$ &  & $x$ & $y$ & $P_2(Y = y | X = x)$ &  &  &  &  &  &  \\ \cline{1-2} \cline{4-6}
\multicolumn{1}{l|}{0}   & $1/2$        &  & 0   & 0   & $1/4$                &  &  &  &  &  &  \\
\multicolumn{1}{l|}{1}   & $1/2$        &  & 0   & 1   & $3/4$                &  &  &  &  &  &  \\
                         &              &  & 1   & 0   & $1/4$                &  &  &  &  &  &  \\
                         &              &  & 1   & 1   & $3/4$                &  &  &  &  &  & 
\end{tabular}

For concision, we will abbreviate $P_1(X =  x, Y = 1 | S = 1)$ as $P_1(x, y | S = 1)$ (and similar for other quantities). Then, by Bayes rule and the law of total probability we have that:

\begin{align*}
P_1(x, y | S = 1) &= \frac{P_1(S = 1| x, y)P_1(x,y)}{P_1(S = 1)} \\ 
&= \frac{P_1(S = 1| x, y)P_1(x,y)}{\sum_{x', y' \in \{0, 1\}} P_1(S =1 | x', y') P_1(x', y')} \\
&= \frac{P_1(S = 1| x, y)P_1(x,y)}{\sum_{x', y' \in \{0, 1\}} P_1(S =1 | x', y') P_1(x', y')} \\
\end{align*}

So, d-separation (and therefore conditional independence) we get:

\begin{align*}
P_1(x, y | S = 1) &= \frac{P_1(S = 1| x, y)P(x,y)}{\sum_{x', y' \in \{0, 1\}} P_1(S =1 | x', y') P_1(x', y')} \\ 
&= \frac{P_2(S = 1| y)P_2(y | x)P_2(x)}{\sum_{x', y' \in \{0, 1\}} P_2(S =1 | y') P_1(y' | x') P_1(x')} 
\end{align*}

Then, since we have defined $P_1(y | x)$ and $P_1(x)$ to be always be $\frac{1}{2}$ we get these terms to cancel, leaving $\sum_{x', y' \in \{0, 1\}} P_1(S =1 | y') = 2$ in the denominator. So, 

$$P_1(x, y | S = 1) = \frac{P_1(S = 1 | y)}{2}.$$

The table for $P_1(x, y | S = 1)$ is then: \\

\begin{tabular}{l|l|lllllll}
$x$ & $y$ & $P_1( y , x | S = 1) =  \frac{P_1(S = 1 | y)}{2}$ &  &  &  &  &  &  \\ \cline{1-3}
0   & 0   & $1/8$                        &  &  &  &  &  &  \\
0   & 1   & $3/8$                        &  &  &  &  &  &  \\
1   & 0   & $1/8$                        &  &  &  &  &  &  \\
1   & 1   & $3/8$                        &  &  &  &  &  & 
\end{tabular}

Now we move on to $P_2$. Since there is no path between $S$ and $X$ or $Y$ in $G_2$, we have that:

$$P_2(y, x | S = 1) = P_2(y,x) =  P_2(y | x)P_2(x) = \frac{P_2(y | x)}{2}$$

This gives the table: \\

\begin{tabular}{l|l|lllllll}
$x$ & $y$ & $P_2(y ,  x | S = 1) =  \frac{P_2(y | x)}{2}$ &  &  &  &  &  &  \\ \cline{1-3}
0   & 0   & $1/8$                        &  &  &  &  &  &  \\
0   & 1   & $3/8$                        &  &  &  &  &  &  \\
1   & 0   & $1/8$                        &  &  &  &  &  &  \\
1   & 1   & $3/8$                        &  &  &  &  &  & 
\end{tabular}

So, $P_1(Y = y | X = x | S = 1) = P_2(Y = y | X = x | S = 1)$ for all $x,y \in \{0,1\}$. But by consulting the original tables, we can see that $P_1(y | x) \neq P_2(y | x)$. Then $G_1$ is not s-recoverable.
\end{document}
 